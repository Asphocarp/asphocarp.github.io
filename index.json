[{"content":"Thoughts Liang says that the output of LLMs can be high-dimensional, as it can be a program / Turing machine, which includes functions and models. This could introduce unique security challenges.\nThis makes me think of two important properties of LLM:\nhigh-dimensional output input and output in the same space enabling interaction/collaboration between agents. (same rank) the output can be further executed by other LLMs. (diff rank) Trending Big Topics:\nTypical AI Security on LLM (relatively) common AI-inherent vulnerabilities adv. examples, backdoor … Language Agents [1] agent interaction/collaboration (dynamics) demos by OpenAI GPT4o in which 1. two agents talk to each other 2. one agent describes what it sees for the other one without vision LLM OS figure by Andrej Karpathy from OpenAI [2] interaction between other LLMs/Agents at the lower right of the figure AI assistant Trade-off among topics:\nthe typical topics are more specific, practical, and straightforward, while the emerging ones are more inspiring, novel, and foundational. between vision and text: vision - continuous, easier. text - discrete, harder, but currently more powerful. between large and small: the larger, the more proprietary, computationally consuming, and hard. For the “Typical AI Security for LLM” topic Despite years of research, the effectiveness of many methods (when applied to LLM) is limited or requires enhancement, due to challenges posed by\ndiscrete optimization difficulties, proprietary models, and inadequate computational resources. Consequently, substantial further work persists in this area. Specific works could be:\ntuning the LLM to output vulnerable codes. the vulnerability introduced by multimodality. adv. example [work by carlini] For the “Security in Agent Interactions” topic We can get extensive novel intuitions from human society. Similar to human interactions, agent dynamics involve multiple facets:\nModels can debate [8] Improving Factuality and Reasoning in Language Models with Multiagent Debate, ICML 2024 by Yilun Du from MIT collaborate [9] Composing Ensembles of Pre-trained Models via Iterative Consensus, ICLR 2022 by Shuang Li and Yilun Du from MIT CSAIL One model may deceive another [7] One model can tune another (alignment, watermarking [my previous work] …) alignment with humans (out of scope though) [?] alignment with other models [3] alignment with itself [4] Hierarchy analogous to human society structure (\u0026gt; denotes “higher than”) small \u0026gt; large: super alignment by OpenAI [10], which can be seen as Human (small model in the future) \u0026gt; LLM, or on-experts managing experts. large \u0026gt; small: in a work called LLMs as Tool Makers [6], larger models make reusable Python tools for small models to run, with the aim of improved efficiency. LLMs could have their own lang for communication in the work of Tool Makers [6] as I mentioned above, the lang for communication is manually set to be Python scripts, which may not be the optimal choice. Others (imagination) one model can create incentives for increased effort through misleading promises As an exploration of this topic (Language Agent Interaction Security), a specific work could be\nattack existing agent interaction frameworks [6, 8] via mischievous participants Tool Makers [6] The attack result can be: the tool made by an adversarial large model is semantically/mathematically right/benign, but the target small models behave mischievously while using this tool. Multiagent Debate [8] The attack result can be: one mischievous participant leads to harmful reasoning and conclusion. Summary Generally speaking, my next research topic is to be included in “LLM Security” because I want to learn more about Transformer, discrete optimization, and AI-inherent vulnerabilities and threats.\nThe specific topic could be practical and straightforward as ”Improving Existing Attacks/Defenses for LLMs”, or novel as ”Security in Language Agent Interactions”.\nI plan to first explore security in language agent interactions, and if too challenging, fall back to refine attacks/defenses for LLMs.\nPlan (try my best to produce a good paper on LLM Security in a year)\n1. Survey read a lot in LLM Security (LLM Agent, Discrete Optimization, Agent Interaction Frameworks, Typical AI Security for LLM); try to unify some existing agent interaction frameworks; define the problem better to align with existing security issues. during 24-05 ~ 24-07 (roughly)\n2. Experiment try to find a common vulnerability in Agent Interaction if too hard, fallback to refine typical attacks/defenses for LLMs during 24-07 ~ 24-12\n3. Write and so on write the paper and keep doing experiments/reading. during 24-12 ~ 25-6\nReference Language Agents: From Next-Token Prediction to Digital Automation, PhD Thesis of Princeton. by Shunyu Yao from Princeton https://twitter.com/karpathy/status/1723140519554105733 by Karpathy from OpenAI Rrhf: Rank responses to align language models with human feedback without tears, NeurIPS 2023 by Zhen Yuan and Hongyi Yuan from Tsinghua and Alibaba DAMO Principle-driven self-alignment of language models from scratch with minimal human supervision, NeurIPS 2023 (Spotlight) by Zhiqing Sun from CMU Neural Network Diffusion by Kai Wang from NUS Large Language Models as Tool Makers, ICLR by Tianle Cai from Princeton and Google Deepmind An LLM can Fool Itself: A Prompt-Based Adversarial Attack, ICLR 2024 (Poster) by Xilie Xu and Keyi Kong from NUS Improving Factuality and Reasoning in Language Models with Multiagent Debate, ICML 2024 by Yilun Du from MIT CSAIL Composing Ensembles of Pre-trained Models via Iterative Consensus, ICLR 2023 by Shuang Li and Yilun Du from MIT CSAIL TODO more collaboration examples openai.com/superalignment/ ","permalink":"https://asphocarp.github.io/posts/research-topics-and-plan-encoded/","summary":"\u003ch2 id=\"thoughts\"\u003eThoughts\u003c/h2\u003e\n\u003cp\u003eLiang says that the output of LLMs can be high-dimensional, as it can be a program / Turing machine, which includes functions and models.\nThis could introduce unique security challenges.\u003c/p\u003e\n\u003cp\u003eThis makes me think of two important properties of LLM:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ehigh-dimensional output\u003c/li\u003e\n\u003cli\u003einput and output in the same space\n\u003cul\u003e\n\u003cli\u003eenabling interaction/collaboration between agents. (same rank)\u003c/li\u003e\n\u003cli\u003ethe output can be further executed by other LLMs. (diff rank)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTrending Big Topics:\u003c/p\u003e","title":"Research Topics and Plan"}]