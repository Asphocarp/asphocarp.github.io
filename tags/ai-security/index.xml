<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI Security on Jyu&#39;Log</title>
    <link>http://localhost:1313/tags/ai-security/</link>
    <description>Recent content in AI Security on Jyu&#39;Log</description>
    <image>
      <title>Jyu&#39;Log</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.140.2</generator>
    <language>en</language>
    <lastBuildDate>Fri, 09 Dec 2022 19:53:33 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ai-security/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reading about Adversarial Examples</title>
      <link>http://localhost:1313/posts/adv/</link>
      <pubDate>Fri, 09 Dec 2022 19:53:33 +0530</pubDate>
      <guid>http://localhost:1313/posts/adv/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Adversarial Examples: Attacks and Defenses for Deep Learning
&lt;ul&gt;
&lt;li&gt;10.48550/arXiv.1712.07107&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;讨论范围：
&lt;ul&gt;
&lt;li&gt;主要针对图像分类/物体识别的对抗样本&lt;/li&gt;
&lt;li&gt;DNN 和 传统机器学习(SVM, Random Forest)&lt;/li&gt;
&lt;li&gt;只考虑了 2017 年 11 月之前发表的论文&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文结构：(各个Section的内容)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;导论&lt;/li&gt;
&lt;li&gt;背景
介绍深度学习技巧、模型、数据集
讨论针对传统机器学习的对抗样本&lt;/li&gt;
&lt;li&gt;对抗样本分类法
&lt;ol&gt;
&lt;li&gt;威胁模型&lt;/li&gt;
&lt;li&gt;扰动&lt;/li&gt;
&lt;li&gt;基准&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;对抗样本生成方法&lt;/li&gt;
&lt;li&gt;对抗样本应用&lt;/li&gt;
&lt;li&gt;对抗样本相应对策&lt;/li&gt;
&lt;li&gt;当前挑战和未来研究方向
&lt;ol&gt;
&lt;li&gt;对抗样本可迁移性&lt;/li&gt;
&lt;li&gt;对抗样本的存在性&lt;/li&gt;
&lt;li&gt;鲁棒性评估&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;总结&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;1-导论&#34;&gt;1. 导论&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;提出了相关威胁模型 (Threat Model)：
&lt;ul&gt;
&lt;li&gt;攻击者只能在测试和部署阶段攻击。
&lt;ul&gt;
&lt;li&gt;不能修改训练数据 (data poisoning)，修改模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;专注于DNN和传统机器学习。
&lt;ul&gt;
&lt;li&gt;针对DNN的也对传统机器学习有效。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;攻击者只损害完整性 (integrity)，用性能指标表示，包括 accuracy, F1 score, AUC。
&lt;ul&gt;
&lt;li&gt;对 confidentiality, privacy 的攻击不做讨论。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;关于何为 Threat Model
&lt;ul&gt;
&lt;li&gt;个人理解：关于攻击者的知识、目的、特点的一系列假设&lt;/li&gt;
&lt;li&gt;威胁建模的目的是根据系统的性质、可能的攻击者概况、最可能的攻击方向以及攻击者最需要的资产，&lt;/li&gt;
&lt;li&gt;威胁建模回答下面这些问题：
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;“Where am I most vulnerable to attack?”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“What are the most relevant threats?”&lt;/em&gt; &lt;/li&gt;
&lt;li&gt;&lt;em&gt;“What do I need to do to safeguard against these threats?”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;威胁建模可以使用下面五种方法中的一种实施：
&lt;ol&gt;
&lt;li&gt;asset-centric&lt;/li&gt;
&lt;li&gt;attacker-centric&lt;/li&gt;
&lt;li&gt;software-centric&lt;/li&gt;
&lt;li&gt;value and stakeholder-centric&lt;/li&gt;
&lt;li&gt;hybrid&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-背景&#34;&gt;2. 背景&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;深度学习常见类型
&lt;ul&gt;
&lt;li&gt;CNN, Convolutional Neural Network
&lt;ul&gt;
&lt;li&gt;适用于图像分类，对象检测，图形分割&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RNN, Recurrent Neural Network
&lt;ul&gt;
&lt;li&gt;解决梯度消失/爆炸
&lt;ul&gt;
&lt;li&gt;LSTM&lt;/li&gt;
&lt;li&gt;GRU&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;深度学习常见架构
&lt;ul&gt;
&lt;li&gt;LeNet、VGG、AlexNet、GoogLeNet、ResNet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;深度学习标准数据集
&lt;ul&gt;
&lt;li&gt;MNIST、CIFAR-10、ImageNet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;有人对机器学习攻击在三个维度上分类：
&lt;ol&gt;
&lt;li&gt;影响：是否需要毒化训练数据&lt;/li&gt;
&lt;li&gt;安全违规：对抗样本是属于假阳性还是假阴性&lt;/li&gt;
&lt;li&gt;特异性：攻击针对特定实例还是一大类&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;在图像分类上的对抗样本攻击最终成为约束问题：
&lt;ul&gt;
&lt;li&gt;在实现分类错误的约束下，最小化扰动。&lt;/li&gt;
&lt;li&gt;$ \begin{array}{ll}\min\limits_{x&amp;rsquo;}&amp;amp;|x&amp;rsquo;-x|\ s.t.&amp;amp;f(x&amp;rsquo;)=l&amp;rsquo;,\ &amp;amp;f(x)=l,\ &amp;amp;l\neq l&amp;rsquo;,\ &amp;amp;x&amp;rsquo;\in[0,1].\end{array} $&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;3-对抗样本分类法&#34;&gt;3. 对抗样本分类法&lt;/h2&gt;
&lt;p&gt;系统性分析对抗样本，从三个维度：威胁模型、扰动和基准&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
