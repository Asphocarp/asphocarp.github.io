<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reading about Adversarial Examples | Jyu'Log</title>
<meta name=keywords content="AI Security,Tech"><meta name=description content="
Adversarial Examples: Attacks and Defenses for Deep Learning

10.48550/arXiv.1712.07107


讨论范围：

主要针对图像分类/物体识别的对抗样本
DNN 和 传统机器学习(SVM, Random Forest)
只考虑了 2017 年 11 月之前发表的论文



论文结构：(各个Section的内容)

导论
背景
介绍深度学习技巧、模型、数据集
讨论针对传统机器学习的对抗样本
对抗样本分类法

威胁模型
扰动
基准


对抗样本生成方法
对抗样本应用
对抗样本相应对策
当前挑战和未来研究方向

对抗样本可迁移性
对抗样本的存在性
鲁棒性评估


总结

1. 导论

提出了相关威胁模型 (Threat Model)：

攻击者只能在测试和部署阶段攻击。

不能修改训练数据 (data poisoning)，修改模型。


专注于DNN和传统机器学习。

针对DNN的也对传统机器学习有效。


攻击者只损害完整性 (integrity)，用性能指标表示，包括 accuracy, F1 score, AUC。

对 confidentiality, privacy 的攻击不做讨论。




关于何为 Threat Model

个人理解：关于攻击者的知识、目的、特点的一系列假设
威胁建模的目的是根据系统的性质、可能的攻击者概况、最可能的攻击方向以及攻击者最需要的资产，
威胁建模回答下面这些问题：

“Where am I most vulnerable to attack?”
“What are the most relevant threats?” 
“What do I need to do to safeguard against these threats?”


威胁建模可以使用下面五种方法中的一种实施：

asset-centric
attacker-centric
software-centric
value and stakeholder-centric
hybrid





2. 背景

深度学习常见类型

CNN, Convolutional Neural Network

适用于图像分类，对象检测，图形分割


RNN, Recurrent Neural Network

解决梯度消失/爆炸

LSTM
GRU






深度学习常见架构

LeNet、VGG、AlexNet、GoogLeNet、ResNet


深度学习标准数据集

MNIST、CIFAR-10、ImageNet


有人对机器学习攻击在三个维度上分类：

影响：是否需要毒化训练数据
安全违规：对抗样本是属于假阳性还是假阴性
特异性：攻击针对特定实例还是一大类


在图像分类上的对抗样本攻击最终成为约束问题：

在实现分类错误的约束下，最小化扰动。
$ \begin{array}{ll}\min\limits_{x&rsquo;}&|x&rsquo;-x|\ s.t.&amp;f(x&rsquo;)=l&rsquo;,\ &amp;f(x)=l,\ &amp;l\neq l&rsquo;,\ &amp;x&rsquo;\in[0,1].\end{array} $



3. 对抗样本分类法
系统性分析对抗样本，从三个维度：威胁模型、扰动和基准"><meta name=author content="Asilvorcarp"><link rel=canonical href=http://localhost:1313/posts/adv/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/adv/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/adv/"><meta property="og:site_name" content="Jyu'Log"><meta property="og:title" content="Reading about Adversarial Examples"><meta property="og:description" content=" Adversarial Examples: Attacks and Defenses for Deep Learning 10.48550/arXiv.1712.07107 讨论范围： 主要针对图像分类/物体识别的对抗样本 DNN 和 传统机器学习(SVM, Random Forest) 只考虑了 2017 年 11 月之前发表的论文 论文结构：(各个Section的内容)
导论 背景 介绍深度学习技巧、模型、数据集 讨论针对传统机器学习的对抗样本 对抗样本分类法 威胁模型 扰动 基准 对抗样本生成方法 对抗样本应用 对抗样本相应对策 当前挑战和未来研究方向 对抗样本可迁移性 对抗样本的存在性 鲁棒性评估 总结 1. 导论 提出了相关威胁模型 (Threat Model)： 攻击者只能在测试和部署阶段攻击。 不能修改训练数据 (data poisoning)，修改模型。 专注于DNN和传统机器学习。 针对DNN的也对传统机器学习有效。 攻击者只损害完整性 (integrity)，用性能指标表示，包括 accuracy, F1 score, AUC。 对 confidentiality, privacy 的攻击不做讨论。 关于何为 Threat Model 个人理解：关于攻击者的知识、目的、特点的一系列假设 威胁建模的目的是根据系统的性质、可能的攻击者概况、最可能的攻击方向以及攻击者最需要的资产， 威胁建模回答下面这些问题： “Where am I most vulnerable to attack?” “What are the most relevant threats?” “What do I need to do to safeguard against these threats?” 威胁建模可以使用下面五种方法中的一种实施： asset-centric attacker-centric software-centric value and stakeholder-centric hybrid 2. 背景 深度学习常见类型 CNN, Convolutional Neural Network 适用于图像分类，对象检测，图形分割 RNN, Recurrent Neural Network 解决梯度消失/爆炸 LSTM GRU 深度学习常见架构 LeNet、VGG、AlexNet、GoogLeNet、ResNet 深度学习标准数据集 MNIST、CIFAR-10、ImageNet 有人对机器学习攻击在三个维度上分类： 影响：是否需要毒化训练数据 安全违规：对抗样本是属于假阳性还是假阴性 特异性：攻击针对特定实例还是一大类 在图像分类上的对抗样本攻击最终成为约束问题： 在实现分类错误的约束下，最小化扰动。 $ \begin{array}{ll}\min\limits_{x’}&|x’-x|\ s.t.&amp;f(x’)=l’,\ &amp;f(x)=l,\ &amp;l\neq l’,\ &amp;x’\in[0,1].\end{array} $ 3. 对抗样本分类法 系统性分析对抗样本，从三个维度：威胁模型、扰动和基准"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-09T19:53:33+05:30"><meta property="article:modified_time" content="2022-12-09T19:53:33+05:30"><meta property="article:tag" content="AI Security"><meta property="article:tag" content="Tech"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Reading about Adversarial Examples"><meta name=twitter:description content="
Adversarial Examples: Attacks and Defenses for Deep Learning

10.48550/arXiv.1712.07107


讨论范围：

主要针对图像分类/物体识别的对抗样本
DNN 和 传统机器学习(SVM, Random Forest)
只考虑了 2017 年 11 月之前发表的论文



论文结构：(各个Section的内容)

导论
背景
介绍深度学习技巧、模型、数据集
讨论针对传统机器学习的对抗样本
对抗样本分类法

威胁模型
扰动
基准


对抗样本生成方法
对抗样本应用
对抗样本相应对策
当前挑战和未来研究方向

对抗样本可迁移性
对抗样本的存在性
鲁棒性评估


总结

1. 导论

提出了相关威胁模型 (Threat Model)：

攻击者只能在测试和部署阶段攻击。

不能修改训练数据 (data poisoning)，修改模型。


专注于DNN和传统机器学习。

针对DNN的也对传统机器学习有效。


攻击者只损害完整性 (integrity)，用性能指标表示，包括 accuracy, F1 score, AUC。

对 confidentiality, privacy 的攻击不做讨论。




关于何为 Threat Model

个人理解：关于攻击者的知识、目的、特点的一系列假设
威胁建模的目的是根据系统的性质、可能的攻击者概况、最可能的攻击方向以及攻击者最需要的资产，
威胁建模回答下面这些问题：

“Where am I most vulnerable to attack?”
“What are the most relevant threats?” 
“What do I need to do to safeguard against these threats?”


威胁建模可以使用下面五种方法中的一种实施：

asset-centric
attacker-centric
software-centric
value and stakeholder-centric
hybrid





2. 背景

深度学习常见类型

CNN, Convolutional Neural Network

适用于图像分类，对象检测，图形分割


RNN, Recurrent Neural Network

解决梯度消失/爆炸

LSTM
GRU






深度学习常见架构

LeNet、VGG、AlexNet、GoogLeNet、ResNet


深度学习标准数据集

MNIST、CIFAR-10、ImageNet


有人对机器学习攻击在三个维度上分类：

影响：是否需要毒化训练数据
安全违规：对抗样本是属于假阳性还是假阴性
特异性：攻击针对特定实例还是一大类


在图像分类上的对抗样本攻击最终成为约束问题：

在实现分类错误的约束下，最小化扰动。
$ \begin{array}{ll}\min\limits_{x&rsquo;}&|x&rsquo;-x|\ s.t.&amp;f(x&rsquo;)=l&rsquo;,\ &amp;f(x)=l,\ &amp;l\neq l&rsquo;,\ &amp;x&rsquo;\in[0,1].\end{array} $



3. 对抗样本分类法
系统性分析对抗样本，从三个维度：威胁模型、扰动和基准"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Reading about Adversarial Examples","item":"http://localhost:1313/posts/adv/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reading about Adversarial Examples","name":"Reading about Adversarial Examples","description":" Adversarial Examples: Attacks and Defenses for Deep Learning 10.48550/arXiv.1712.07107 讨论范围： 主要针对图像分类/物体识别的对抗样本 DNN 和 传统机器学习(SVM, Random Forest) 只考虑了 2017 年 11 月之前发表的论文 论文结构：(各个Section的内容)\n导论 背景 介绍深度学习技巧、模型、数据集 讨论针对传统机器学习的对抗样本 对抗样本分类法 威胁模型 扰动 基准 对抗样本生成方法 对抗样本应用 对抗样本相应对策 当前挑战和未来研究方向 对抗样本可迁移性 对抗样本的存在性 鲁棒性评估 总结 1. 导论 提出了相关威胁模型 (Threat Model)： 攻击者只能在测试和部署阶段攻击。 不能修改训练数据 (data poisoning)，修改模型。 专注于DNN和传统机器学习。 针对DNN的也对传统机器学习有效。 攻击者只损害完整性 (integrity)，用性能指标表示，包括 accuracy, F1 score, AUC。 对 confidentiality, privacy 的攻击不做讨论。 关于何为 Threat Model 个人理解：关于攻击者的知识、目的、特点的一系列假设 威胁建模的目的是根据系统的性质、可能的攻击者概况、最可能的攻击方向以及攻击者最需要的资产， 威胁建模回答下面这些问题： “Where am I most vulnerable to attack?” “What are the most relevant threats?” “What do I need to do to safeguard against these threats?” 威胁建模可以使用下面五种方法中的一种实施： asset-centric attacker-centric software-centric value and stakeholder-centric hybrid 2. 背景 深度学习常见类型 CNN, Convolutional Neural Network 适用于图像分类，对象检测，图形分割 RNN, Recurrent Neural Network 解决梯度消失/爆炸 LSTM GRU 深度学习常见架构 LeNet、VGG、AlexNet、GoogLeNet、ResNet 深度学习标准数据集 MNIST、CIFAR-10、ImageNet 有人对机器学习攻击在三个维度上分类： 影响：是否需要毒化训练数据 安全违规：对抗样本是属于假阳性还是假阴性 特异性：攻击针对特定实例还是一大类 在图像分类上的对抗样本攻击最终成为约束问题： 在实现分类错误的约束下，最小化扰动。 $ \\begin{array}{ll}\\min\\limits_{x\u0026rsquo;}\u0026amp;|x\u0026rsquo;-x|\\ s.t.\u0026amp;f(x\u0026rsquo;)=l\u0026rsquo;,\\ \u0026amp;f(x)=l,\\ \u0026amp;l\\neq l\u0026rsquo;,\\ \u0026amp;x\u0026rsquo;\\in[0,1].\\end{array} $ 3. 对抗样本分类法 系统性分析对抗样本，从三个维度：威胁模型、扰动和基准\n","keywords":["AI Security","Tech"],"articleBody":" Adversarial Examples: Attacks and Defenses for Deep Learning 10.48550/arXiv.1712.07107 讨论范围： 主要针对图像分类/物体识别的对抗样本 DNN 和 传统机器学习(SVM, Random Forest) 只考虑了 2017 年 11 月之前发表的论文 论文结构：(各个Section的内容)\n导论 背景 介绍深度学习技巧、模型、数据集 讨论针对传统机器学习的对抗样本 对抗样本分类法 威胁模型 扰动 基准 对抗样本生成方法 对抗样本应用 对抗样本相应对策 当前挑战和未来研究方向 对抗样本可迁移性 对抗样本的存在性 鲁棒性评估 总结 1. 导论 提出了相关威胁模型 (Threat Model)： 攻击者只能在测试和部署阶段攻击。 不能修改训练数据 (data poisoning)，修改模型。 专注于DNN和传统机器学习。 针对DNN的也对传统机器学习有效。 攻击者只损害完整性 (integrity)，用性能指标表示，包括 accuracy, F1 score, AUC。 对 confidentiality, privacy 的攻击不做讨论。 关于何为 Threat Model 个人理解：关于攻击者的知识、目的、特点的一系列假设 威胁建模的目的是根据系统的性质、可能的攻击者概况、最可能的攻击方向以及攻击者最需要的资产， 威胁建模回答下面这些问题： “Where am I most vulnerable to attack?” “What are the most relevant threats?” “What do I need to do to safeguard against these threats?” 威胁建模可以使用下面五种方法中的一种实施： asset-centric attacker-centric software-centric value and stakeholder-centric hybrid 2. 背景 深度学习常见类型 CNN, Convolutional Neural Network 适用于图像分类，对象检测，图形分割 RNN, Recurrent Neural Network 解决梯度消失/爆炸 LSTM GRU 深度学习常见架构 LeNet、VGG、AlexNet、GoogLeNet、ResNet 深度学习标准数据集 MNIST、CIFAR-10、ImageNet 有人对机器学习攻击在三个维度上分类： 影响：是否需要毒化训练数据 安全违规：对抗样本是属于假阳性还是假阴性 特异性：攻击针对特定实例还是一大类 在图像分类上的对抗样本攻击最终成为约束问题： 在实现分类错误的约束下，最小化扰动。 $ \\begin{array}{ll}\\min\\limits_{x’}\u0026|x’-x|\\ s.t.\u0026f(x’)=l’,\\ \u0026f(x)=l,\\ \u0026l\\neq l’,\\ \u0026x’\\in[0,1].\\end{array} $ 3. 对抗样本分类法 系统性分析对抗样本，从三个维度：威胁模型、扰动和基准\n威胁模型 对抗性伪造 假阳性 - 例如良性软件被误判为恶意软件，人类不可识别图像被识别为高置信度的某个类别 假阴性 - 也称逃逸，例如恶意软件不被发现 攻击者知识 白盒 - 知道一切：训练数据、模型架构、超参数、模型参数 黑盒 - 只知道输出（标签/置信度）。通过对抗样本的可迁移性实现。 对抗特异性 有目标 - 最大化目标分类的概率 无目标 - 更易实施，可以通过两种方式 多次有目标攻击，取扰动最小的 最小化正确分类的概率 也有 extended BIM / ZOO 同时适用有目标/无目标 攻击频率 One-time - 只优化一次对抗样本 Iterative - 可以多次更新迭代对抗样本 对以强化学习为例的计算密集型任务，一次性攻击可能是唯一选择。 扰动 扰动范围 Individual 单独 - 对每个干净输入单独产生扰动 Universal 普适 - 产生适用于整个数据集的扰动 目前大多是单独扰动，但普适扰动可以让实际部署更简单 扰动限制 最优扰动 - 直到最小 限制扰动 - 足够小即可 扰动测量 p-norm distance $|x|p=\\left(\\sum\\limits{i=1}^n|x_i|^p\\right)^\\frac{1}{p}$ 常用 $\\ell_0,\\ell_2,\\ell_\\infty$ PASS, psychometric perceptual adversarial similarity 从人类感知角度衡量 基准 常用数据集：MNIST、CIFAR-10、ImageNet 目前最佳是ImageNet，其他的太易攻破 受害者模型：LeNet、VGG、AlexNet、GoogLeNet、CaffeNet、ResNet 分类法图示： 4. 对抗样本生成方法 (从图像分类的角度考虑)\n各种对抗样本生成方法 L-BFGS 使用昂贵的线性搜索方法来寻找最优值，费时不切实际。 FGSM, Fast Gradient Sign Method 每个像素执行一次梯度更新。 扰动大小： $\\eta=\\epsilon sign(\\nabla_x J_\\theta(x,l))$ 可以通过反向传播计算。 论文指出一步攻击易于转移但也易于防御 于是提出可以引入动量 最大化目标类概率的FGSM 称作 OTCM, one-step target class method 发现由于梯度掩蔽(gradient masking)经过对抗训练的FGSM对白盒更鲁棒 于是提出在更新对抗样本时加入随机性来战胜对抗训练，称作 RAND-FGSM BIM (Basic Iterative Method) \u0026 ILLC (Iterative Least-Likely Class Method) 拓展FGSM来使对抗样本更好适用于物理世界。 发现FGSM对光变换(phototransformation)鲁棒，而迭代方法则不。 JSMA, Jacobian-based Saliency Map Attack 计算给定样本x的Jacobian matrix $J_F(x)=\\dfrac{\\partial F(x)}{\\partial x}=\\left[\\dfrac{\\partial F_j(x)}{\\partial x_i}\\right]_{i\\times j}$ 由此找出对输出影响最大的x的输入特征 计算成本巨大，运行缓慢。 DeepFool 与 FGSM 和 JSMA 相比，DeepFool 提供的扰动更少。与 JSMA 相比，DeepFool 还减少了扰动的强度而不是所选特征的数量。 CPPN EA Fool, compositional pattern-producing network-encoded EA EA, evolutionary algorithms 进化算法 生成假阳性对抗样本 可以像 JSMA 一样找到关键特征来改变深度神经网络的输出 C\u0026W’s Attack ZOO, Zeroth Order Optimization 不需要梯度，可以直接部署在黑盒攻击中而无需模型传输。 需要昂贵的计算来查询和估计梯度。 也提出了ZOO-ADAM，以随机选择一个变量并更新对抗性示例。 实验表明，ZOO 实现了与 C\u0026W 的 Attack 相当的性能。 Universal Perturbation 每次迭代使用 DeepFool 方法针对每个输入数据获得最小样本扰动，并将扰动更新为总扰动 η。直到大多数数据样本被愚弄 (P \u003c 1 − δ)，这个循环才会停止。 One Pixel Attack 只改一个像素，优化问题变成： $\\begin{array}{ll}\\min\\limits_{x’}\u0026J(f(x’),l’)\\ s.t.\u0026|\\eta|_0\\le\\epsilon_0\\end{array}$ 但没有梯度难以优化。 于是用 DE (differential evolution 差分进化) 找到最优解。 Feature Addversary 最小化 内部神经网络层而不是输出层 的表示距离来进行有目标攻击。 $\\begin{array}{ll}\\min\\limits_{x’}\u0026|\\phi_k(x)-\\phi_k(x’)|\\ s.t.\u0026|x-x’|_\\infty\u003c\\delta\\end{array}$ 其中 $\\phi_k$ 表示从图像输入到第 k 层输出的映射。 不寻找最小扰动，而是用 $\\delta$ 约束扰动。 Hot/Cold 用到了心理测量知觉对抗性相似性评分 (PASS)，Structural SIMilarity (SSIM)。 为了生成一组不同的对抗性示例，将目标标签 l’ 定义为热类，将原始标签 l 定义为冷类。在每次迭代中，在远离原始（冷）类的同时移向目标（热）类。 结果表明，生成的对抗样本与 FGSM 相当，并且具有更多的多样性。 Natural GAN 利用生成对抗网络 (GAN) 作为生成图像和文本的对抗性示例的方法的一部分，使得对抗性示例对人类来说更加自然。 Natural GAN 是许多深度学习领域的通用框架，可应用于图像分类、文本蕴含和机器翻译。 Natural GAN不需要原始神经网络的梯度，因此也可以应用于黑盒攻击。 Model-based Ensembling Attack 基于模型的集成攻击 对多个模型进行白盒攻击，然后在黑盒上测试。 生成可转移的对抗样本，增强黑盒攻击威力。 优化问题： $\\underset{x’}{\\operatorname{argmin}}\\quad-\\log\\bigg((\\sum\\limits_{i=1}^k\\alpha_i J_i(x’,l’))\\bigg)+\\lambda|x’-x|$ where $k$ is the number of deep neural networks in the generation, $f_i$ is the function of each network, and $α_i$ is the ensemble weight ($∑^k_i α_i = 1$). 另外它的 无目标对抗样本 生成效果也比以前的方法更好。 Ground-Truth Attack 给出最小的对抗样本扰动$(\\ell_1,\\ell_\\infty)$。 进行二分搜索，并通过迭代调用 Reluplex 找到了这样一个扰动最小的对抗样本。 最初的对抗样本使用 C\u0026W Attack 来提高性能。 5. 对抗样本应用 除了图像分类的其他场景： 如何生成对抗样本 能否转化成图像分类问题，从而间接解决 Summary Table: 各种场景包括： reinforcement learning 强化学习 都是 One-time attack. 强化学习网络和算法有 DQN, deep Q network TRPO, trust region policy optimization A3C, asynchronous advantage actor-critic 数据集：Atari 使用$\\ell_1$ norm的Huang’s Attack对黑白盒都有效 generative modeling 生成模型 将扰动注入编码器的输入并在解码后生成目标类 face recognition 面部识别 涉及softmaxloss 通过向优化目标添加NPS (non-printablity score)，增强对抗样本的printability $RP_2$, robust physical perturbation 鲁棒物理扰动 object detection 物体检测 DAG, Dense Adversary Generation 适用物体检测/语义分割 还可以生成假阳性样本 semantic segmentation 语义分割 图像分割可以认为是对像素进行图像分类。 攻击思路是把像素分类成与背景同类。 实验表明语义分割存在普遍扰动攻击。 NLP 自然语言处理 通过增减单词生成对抗样本。 实现误导 问答，情感分类等。 malware detection 恶意软件检测 深度学习可以检测出0-day恶意软件，应用广泛。 Android恶意软件 - 使用JSMA攻击 PDF - 使用遗传编程更改对象结构 PE - [102]通过强化学习训练，逃逸率视为奖励。 6. 对抗样本相应对策 两种防御策略 reactive 反应式 在建立深度神经网络后检测对抗样本 proactive 主动 主动增强鲁棒性 reactive Adversarial Detecting 对抗样本检测 在测试阶段检测对抗样本。 但大多数方法无法防御C\u0026W’s Attack。 Input Reconstruction 输入重构 通过重建将对抗样本转换为干净的数据。 Network Verification 网络验证 检查神经网络的属性，并判断输入是否违反或满足属性。可以防御新的没见过的攻击。 一种对具有 ReLU 激活函数的神经网络验证方法，称为 Reluplex。 proactive Network Distillation 模型蒸馏 最初旨在通过将知识从大型网络转移到小型网络来减小深度神经网络的规模。另外也提高了神经网络的泛化能力。 Adversarial (Re)training 对抗训练 使用对抗样本进行训练。 Classifier Robustifying 分类器强化 设计深度神经网络的稳健架构以防止对抗样本。 Ensembling Defenses 集成防御 由于对抗样本多面性(multi-facet)，可以同时用多种防御手段。但有论文[133]指出一些防御方法的集成未能使神经网络更健壮。 7. 当前挑战和未来研究方向 对抗样本可迁移性 从易到难可以分为三个级别： 在使用不同数据训练的相同神经网络架构之间迁移 在为同一任务训练的不同神经网络架构之间迁移 在不同任务的深度神经网络之间迁移 而c级别的迁移目前没有 对抗样本的存在原因 有许多假说： Data incompletion Model capability No robust model 鲁棒性评估 DNN的鲁棒性评估方法 攻防的基准测试平台 各种场景的鲁棒性统一评估 个人思考 对抗样本攻击最后基本都会成为一个优化问题，而防御主要在于增强模型鲁棒性。\n另外如果做对抗样本研究，可以多加考虑标准的图像分类任务以外的领域，比如NLP，恶意软件检测等。\n","wordCount":"566","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2022-12-09T19:53:33+05:30","dateModified":"2022-12-09T19:53:33+05:30","author":{"@type":"Person","name":"Asilvorcarp"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/adv/"},"publisher":{"@type":"Organization","name":"Jyu'Log","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Jyu'Log (Alt + H)"><img src=http://localhost:1313/nope.png alt aria-label=logo height=35>Jyu'Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archive/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Reading about Adversarial Examples
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2022-12-09 19:53:33 +0530 +0530'>December 9, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Asilvorcarp&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/adv.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-导论>1. 导论</a></li><li><a href=#2-背景>2. 背景</a></li><li><a href=#3-对抗样本分类法>3. 对抗样本分类法</a></li><li><a href=#4-对抗样本生成方法>4. 对抗样本生成方法</a></li><li><a href=#5-对抗样本应用>5. 对抗样本应用</a></li><li><a href=#6-对抗样本相应对策>6. 对抗样本相应对策</a></li><li><a href=#7-当前挑战和未来研究方向>7. 当前挑战和未来研究方向</a></li><li><a href=#个人思考>个人思考</a></li></ul></nav></div></details></div><div class=post-content><ul><li>Adversarial Examples: Attacks and Defenses for Deep Learning<ul><li>10.48550/arXiv.1712.07107</li></ul></li><li>讨论范围：<ul><li>主要针对图像分类/物体识别的对抗样本</li><li>DNN 和 传统机器学习(SVM, Random Forest)</li><li>只考虑了 2017 年 11 月之前发表的论文</li></ul></li></ul><p>论文结构：(各个Section的内容)</p><ol><li>导论</li><li>背景
介绍深度学习技巧、模型、数据集
讨论针对传统机器学习的对抗样本</li><li>对抗样本分类法<ol><li>威胁模型</li><li>扰动</li><li>基准</li></ol></li><li>对抗样本生成方法</li><li>对抗样本应用</li><li>对抗样本相应对策</li><li>当前挑战和未来研究方向<ol><li>对抗样本可迁移性</li><li>对抗样本的存在性</li><li>鲁棒性评估</li></ol></li><li>总结</li></ol><h2 id=1-导论>1. 导论<a hidden class=anchor aria-hidden=true href=#1-导论>#</a></h2><ul><li>提出了相关威胁模型 (Threat Model)：<ul><li>攻击者只能在测试和部署阶段攻击。<ul><li>不能修改训练数据 (data poisoning)，修改模型。</li></ul></li><li>专注于DNN和传统机器学习。<ul><li>针对DNN的也对传统机器学习有效。</li></ul></li><li>攻击者只损害完整性 (integrity)，用性能指标表示，包括 accuracy, F1 score, AUC。<ul><li>对 confidentiality, privacy 的攻击不做讨论。</li></ul></li></ul></li><li>关于何为 Threat Model<ul><li>个人理解：关于攻击者的知识、目的、特点的一系列假设</li><li>威胁建模的目的是根据系统的性质、可能的攻击者概况、最可能的攻击方向以及攻击者最需要的资产，</li><li>威胁建模回答下面这些问题：<ul><li><em>“Where am I most vulnerable to attack?”</em></li><li><em>“What are the most relevant threats?”</em> </li><li><em>“What do I need to do to safeguard against these threats?”</em></li></ul></li><li>威胁建模可以使用下面五种方法中的一种实施：<ol><li>asset-centric</li><li>attacker-centric</li><li>software-centric</li><li>value and stakeholder-centric</li><li>hybrid</li></ol></li></ul></li></ul><h2 id=2-背景>2. 背景<a hidden class=anchor aria-hidden=true href=#2-背景>#</a></h2><ul><li>深度学习常见类型<ul><li>CNN, Convolutional Neural Network<ul><li>适用于图像分类，对象检测，图形分割</li></ul></li><li>RNN, Recurrent Neural Network<ul><li>解决梯度消失/爆炸<ul><li>LSTM</li><li>GRU</li></ul></li></ul></li></ul></li><li>深度学习常见架构<ul><li>LeNet、VGG、AlexNet、GoogLeNet、ResNet</li></ul></li><li>深度学习标准数据集<ul><li>MNIST、CIFAR-10、ImageNet</li></ul></li><li>有人对机器学习攻击在三个维度上分类：<ol><li>影响：是否需要毒化训练数据</li><li>安全违规：对抗样本是属于假阳性还是假阴性</li><li>特异性：攻击针对特定实例还是一大类</li></ol></li><li>在图像分类上的对抗样本攻击最终成为约束问题：<ul><li>在实现分类错误的约束下，最小化扰动。</li><li>$ \begin{array}{ll}\min\limits_{x&rsquo;}&|x&rsquo;-x|\ s.t.&amp;f(x&rsquo;)=l&rsquo;,\ &amp;f(x)=l,\ &amp;l\neq l&rsquo;,\ &amp;x&rsquo;\in[0,1].\end{array} $</li></ul></li></ul><h2 id=3-对抗样本分类法>3. 对抗样本分类法<a hidden class=anchor aria-hidden=true href=#3-对抗样本分类法>#</a></h2><p>系统性分析对抗样本，从三个维度：威胁模型、扰动和基准</p><ul><li>威胁模型<ol><li>对抗性伪造
假阳性 - 例如良性软件被误判为恶意软件，人类不可识别图像被识别为高置信度的某个类别
假阴性 - 也称逃逸，例如恶意软件不被发现</li><li>攻击者知识
白盒 - 知道一切：训练数据、模型架构、超参数、模型参数
黑盒 - 只知道输出（标签/置信度）。通过对抗样本的可迁移性实现。</li><li>对抗特异性
有目标 - 最大化目标分类的概率
无目标 - 更易实施，可以通过两种方式<ol><li>多次有目标攻击，取扰动最小的</li><li>最小化正确分类的概率
也有 extended BIM / ZOO 同时适用有目标/无目标</li></ol></li><li>攻击频率
One-time - 只优化一次对抗样本
Iterative - 可以多次更新迭代对抗样本
对以强化学习为例的计算密集型任务，一次性攻击可能是唯一选择。</li></ol></li><li>扰动<ol><li>扰动范围
Individual 单独 - 对每个干净输入单独产生扰动
Universal 普适 - 产生适用于整个数据集的扰动
目前大多是单独扰动，但普适扰动可以让实际部署更简单</li><li>扰动限制
最优扰动 - 直到最小
限制扰动 - 足够小即可</li><li>扰动测量<ul><li>p-norm distance<ul><li>$|x|<em>p=\left(\sum\limits</em>{i=1}^n|x_i|^p\right)^\frac{1}{p}$</li><li>常用 $\ell_0,\ell_2,\ell_\infty$</li></ul></li><li>PASS, psychometric perceptual adversarial similarity<ul><li>从人类感知角度衡量</li></ul></li></ul></li></ol></li><li>基准<ul><li>常用数据集：MNIST、CIFAR-10、ImageNet<ul><li>目前最佳是ImageNet，其他的太易攻破</li></ul></li><li>受害者模型：LeNet、VGG、AlexNet、GoogLeNet、CaffeNet、ResNet</li></ul></li><li>分类法图示：
<img alt=AdvAttackTaxonomy202307021733284 loading=lazy src=https://drive.asc.cool/d/p/AdvAttackTaxonomy202307021733284.png></li></ul><h2 id=4-对抗样本生成方法>4. 对抗样本生成方法<a hidden class=anchor aria-hidden=true href=#4-对抗样本生成方法>#</a></h2><p>(从图像分类的角度考虑)</p><ul><li>各种对抗样本生成方法<ol><li>L-BFGS<ul><li>使用昂贵的线性搜索方法来寻找最优值，费时不切实际。</li></ul></li><li>FGSM, Fast Gradient Sign Method<ul><li>每个像素执行一次梯度更新。</li><li>扰动大小：<ul><li>$\eta=\epsilon sign(\nabla_x J_\theta(x,l))$</li><li>可以通过反向传播计算。</li></ul></li><li>论文指出一步攻击易于转移但也易于防御<ul><li>于是提出可以引入动量</li></ul></li><li>最大化目标类概率的FGSM<ul><li>称作 OTCM, one-step target class method</li></ul></li><li>发现由于梯度掩蔽(gradient masking)经过对抗训练的FGSM对白盒更鲁棒<ul><li>于是提出在更新对抗样本时加入随机性来战胜对抗训练，称作 RAND-FGSM</li></ul></li></ul></li><li>BIM (Basic Iterative Method) & ILLC (Iterative Least-Likely Class Method)
拓展FGSM来使对抗样本更好适用于物理世界。
发现FGSM对光变换(phototransformation)鲁棒，而迭代方法则不。</li><li>JSMA, Jacobian-based Saliency Map Attack<ul><li>计算给定样本x的Jacobian matrix<ul><li>$J_F(x)=\dfrac{\partial F(x)}{\partial x}=\left[\dfrac{\partial F_j(x)}{\partial x_i}\right]_{i\times j}$</li><li>由此找出对输出影响最大的x的输入特征</li></ul></li><li>计算成本巨大，运行缓慢。</li></ul></li><li>DeepFool
与 FGSM 和 JSMA 相比，DeepFool 提供的扰动更少。与 JSMA 相比，DeepFool 还减少了扰动的强度而不是所选特征的数量。</li><li>CPPN EA Fool, compositional pattern-producing network-encoded EA
EA, evolutionary algorithms 进化算法
生成假阳性对抗样本
可以像 JSMA 一样找到关键特征来改变深度神经网络的输出</li><li>C&amp;W’s Attack</li><li>ZOO, Zeroth Order Optimization
不需要梯度，可以直接部署在黑盒攻击中而无需模型传输。
需要昂贵的计算来查询和估计梯度。
也提出了ZOO-ADAM，以随机选择一个变量并更新对抗性示例。
实验表明，ZOO 实现了与 C&amp;W 的 Attack 相当的性能。</li><li>Universal Perturbation
每次迭代使用 DeepFool 方法针对每个输入数据获得最小样本扰动，并将扰动更新为总扰动 η。直到大多数数据样本被愚弄 (P &lt; 1 − δ)，这个循环才会停止。</li><li>One Pixel Attack<ul><li>只改一个像素，优化问题变成：<ul><li>$\begin{array}{ll}\min\limits_{x&rsquo;}&amp;J(f(x&rsquo;),l&rsquo;)\ s.t.&|\eta|_0\le\epsilon_0\end{array}$</li><li>但没有梯度难以优化。</li></ul></li><li>于是用 DE (differential evolution 差分进化) 找到最优解。</li></ul></li><li>Feature Addversary<ul><li>最小化 内部神经网络层而不是输出层 的表示距离来进行有目标攻击。<ul><li>$\begin{array}{ll}\min\limits_{x&rsquo;}&|\phi_k(x)-\phi_k(x&rsquo;)|\ s.t.&|x-x&rsquo;|_\infty&lt;\delta\end{array}$</li><li>其中 $\phi_k$ 表示从图像输入到第 k 层输出的映射。</li><li>不寻找最小扰动，而是用 $\delta$ 约束扰动。</li></ul></li></ul></li><li>Hot/Cold
用到了心理测量知觉对抗性相似性评分 (PASS)，Structural SIMilarity (SSIM)。
为了生成一组不同的对抗性示例，将目标标签 l&rsquo; 定义为热类，将原始标签 l 定义为冷类。在每次迭代中，在远离原始（冷）类的同时移向目标（热）类。
结果表明，生成的对抗样本与 FGSM 相当，并且具有更多的多样性。</li><li>Natural GAN
利用生成对抗网络 (GAN) 作为生成图像和文本的对抗性示例的方法的一部分，使得对抗性示例对人类来说更加自然。
Natural GAN 是许多深度学习领域的通用框架，可应用于图像分类、文本蕴含和机器翻译。
Natural GAN不需要原始神经网络的梯度，因此也可以应用于黑盒攻击。</li><li>Model-based Ensembling Attack<ul><li>基于模型的集成攻击<ul><li>对多个模型进行白盒攻击，然后在黑盒上测试。</li><li>生成可转移的对抗样本，增强黑盒攻击威力。</li></ul></li><li>优化问题：<ul><li>$\underset{x&rsquo;}{\operatorname{argmin}}\quad-\log\bigg((\sum\limits_{i=1}^k\alpha_i J_i(x&rsquo;,l&rsquo;))\bigg)+\lambda|x&rsquo;-x|$</li><li>where $k$ is the number of deep neural networks in the generation, $f_i$ is the function of each network, and $α_i$ is the ensemble weight ($∑^k_i α_i = 1$).</li><li>另外它的 无目标对抗样本 生成效果也比以前的方法更好。</li></ul></li></ul></li><li>Ground-Truth Attack<ul><li>给出最小的对抗样本扰动$(\ell_1,\ell_\infty)$。</li><li>进行二分搜索，并通过迭代调用 Reluplex 找到了这样一个扰动最小的对抗样本。</li><li>最初的对抗样本使用 C&amp;W Attack 来提高性能。</li></ul></li></ol></li></ul><h2 id=5-对抗样本应用>5. 对抗样本应用<a hidden class=anchor aria-hidden=true href=#5-对抗样本应用>#</a></h2><ul><li>除了图像分类的其他场景：<ul><li>如何生成对抗样本</li><li>能否转化成图像分类问题，从而间接解决</li></ul></li><li>Summary Table:
<img alt=AdvSummaryTable202307021734921 loading=lazy src=https://drive.asc.cool/d/p/AdvSummaryTable202307021734921.png></li><li>各种场景包括：<ol><li>reinforcement learning 强化学习<ul><li>都是 One-time attack.</li><li>强化学习网络和算法有<ul><li>DQN, deep Q network</li><li>TRPO, trust region policy optimization</li><li>A3C, asynchronous advantage actor-critic</li></ul></li><li>数据集：Atari</li><li>使用$\ell_1$ norm的Huang’s Attack对黑白盒都有效</li></ul></li><li>generative modeling 生成模型
将扰动注入编码器的输入并在解码后生成目标类</li><li>face recognition 面部识别<ul><li>涉及softmaxloss</li><li>通过向优化目标添加NPS (non-printablity score)，增强对抗样本的printability</li><li>$RP_2$, robust physical perturbation 鲁棒物理扰动</li></ul></li><li>object detection 物体检测<ul><li>DAG, Dense Adversary Generation<ul><li>适用物体检测/语义分割</li><li>还可以生成假阳性样本</li></ul></li></ul></li><li>semantic segmentation 语义分割<ul><li>图像分割可以认为是对像素进行图像分类。</li><li>攻击思路是把像素分类成与背景同类。</li><li>实验表明语义分割存在普遍扰动攻击。</li></ul></li><li>NLP 自然语言处理<ul><li>通过增减单词生成对抗样本。</li><li>实现误导 问答，情感分类等。</li></ul></li><li>malware detection 恶意软件检测<ul><li>深度学习可以检测出0-day恶意软件，应用广泛。</li><li>Android恶意软件 - 使用JSMA攻击</li><li>PDF - 使用遗传编程更改对象结构</li><li>PE - [102]通过强化学习训练，逃逸率视为奖励。</li></ul></li></ol></li></ul><h2 id=6-对抗样本相应对策>6. 对抗样本相应对策<a hidden class=anchor aria-hidden=true href=#6-对抗样本相应对策>#</a></h2><ul><li>两种防御策略<ol><li>reactive 反应式
在建立深度神经网络后检测对抗样本</li><li>proactive 主动
主动增强鲁棒性</li></ol></li><li>reactive<ol><li>Adversarial Detecting 对抗样本检测
在测试阶段检测对抗样本。
但大多数方法无法防御C&amp;W’s Attack。</li><li>Input Reconstruction 输入重构
通过重建将对抗样本转换为干净的数据。</li><li>Network Verification 网络验证
检查神经网络的属性，并判断输入是否违反或满足属性。可以防御新的没见过的攻击。
一种对具有 ReLU 激活函数的神经网络验证方法，称为 Reluplex。</li></ol></li><li>proactive<ol><li>Network Distillation 模型蒸馏
最初旨在通过将知识从大型网络转移到小型网络来减小深度神经网络的规模。另外也提高了神经网络的泛化能力。</li><li>Adversarial (Re)training 对抗训练
使用对抗样本进行训练。</li><li>Classifier Robustifying 分类器强化
设计深度神经网络的稳健架构以防止对抗样本。</li><li>Ensembling Defenses 集成防御
由于对抗样本多面性(multi-facet)，可以同时用多种防御手段。但有论文[133]指出一些防御方法的集成未能使神经网络更健壮。</li></ol></li></ul><h2 id=7-当前挑战和未来研究方向>7. 当前挑战和未来研究方向<a hidden class=anchor aria-hidden=true href=#7-当前挑战和未来研究方向>#</a></h2><ol><li>对抗样本可迁移性
从易到难可以分为三个级别：<ol><li>在使用不同数据训练的相同神经网络架构之间迁移</li><li>在为同一任务训练的不同神经网络架构之间迁移</li><li>在不同任务的深度神经网络之间迁移
而c级别的迁移目前没有</li></ol></li><li>对抗样本的存在原因
有许多假说：<ol><li>Data incompletion</li><li>Model capability</li><li>No robust model</li></ol></li><li>鲁棒性评估<ol><li>DNN的鲁棒性评估方法</li><li>攻防的基准测试平台</li><li>各种场景的鲁棒性统一评估</li></ol></li></ol><h2 id=个人思考>个人思考<a hidden class=anchor aria-hidden=true href=#个人思考>#</a></h2><p>对抗样本攻击最后基本都会成为一个优化问题，而防御主要在于增强模型鲁棒性。</p><p>另外如果做对抗样本研究，可以多加考虑标准的图像分类任务以外的领域，比如NLP，恶意软件检测等。</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ai-security/>AI Security</a></li><li><a href=http://localhost:1313/tags/tech/>Tech</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Reading about Adversarial Examples on x" href="https://x.com/intent/tweet/?text=Reading%20about%20Adversarial%20Examples&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f&amp;hashtags=AISecurity%2cTech"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reading about Adversarial Examples on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f&amp;title=Reading%20about%20Adversarial%20Examples&amp;summary=Reading%20about%20Adversarial%20Examples&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reading about Adversarial Examples on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f&title=Reading%20about%20Adversarial%20Examples"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reading about Adversarial Examples on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reading about Adversarial Examples on whatsapp" href="https://api.whatsapp.com/send?text=Reading%20about%20Adversarial%20Examples%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reading about Adversarial Examples on telegram" href="https://telegram.me/share/url?text=Reading%20about%20Adversarial%20Examples&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Reading about Adversarial Examples on ycombinator" href="https://news.ycombinator.com/submitlink?t=Reading%20about%20Adversarial%20Examples&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fadv%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Jyu'Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>